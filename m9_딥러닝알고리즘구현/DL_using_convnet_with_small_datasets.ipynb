{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DL_using_convnet_with_small_datasets.ipynb","provenance":[],"mount_file_id":"13fH7u0uOUl9k6vNx2_BTUX_-MSH0S0UL","authorship_tag":"ABX9TyOgD2r5zoI8GfN7mkmWlQLQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"Oy-zHmkE2vms","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649640449375,"user_tz":-540,"elapsed":90191,"user":{"displayName":"이재우","userId":"15413872261520463322"}},"outputId":"ee7bab05-d1ea-4239-de88-e9e3dda30a57"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting keras==2.3.1\n","  Downloading Keras-2.3.1-py2.py3-none-any.whl (377 kB)\n","\u001b[K     |████████████████████████████████| 377 kB 4.9 MB/s \n","\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (3.1.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (3.13)\n","Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.21.5)\n","Collecting keras-applications>=1.0.6\n","  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n","\u001b[K     |████████████████████████████████| 50 kB 5.8 MB/s \n","\u001b[?25hRequirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.4.1)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.15.0)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.1.2)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras==2.3.1) (1.5.2)\n","Installing collected packages: keras-applications, keras\n","  Attempting uninstall: keras\n","    Found existing installation: keras 2.8.0\n","    Uninstalling keras-2.8.0:\n","      Successfully uninstalled keras-2.8.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n","tensorflow 2.8.0 requires keras<2.9,>=2.8.0rc0, but you have keras 2.3.1 which is incompatible.\u001b[0m\n","Successfully installed keras-2.3.1 keras-applications-1.0.8\n","Collecting tensorflow==2.2.0\n","  Downloading tensorflow-2.2.0-cp37-cp37m-manylinux2010_x86_64.whl (516.2 MB)\n","\u001b[K     |████████████████████████████████| 516.2 MB 4.2 kB/s \n","\u001b[?25hRequirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.6.3)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.14.0)\n","Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.4.1)\n","Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (0.2.0)\n","Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.21.5)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (3.3.0)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.0.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.15.0)\n","Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (3.17.3)\n","Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.1.2)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.1.0)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.44.0)\n","Collecting tensorboard<2.3.0,>=2.2.0\n","  Downloading tensorboard-2.2.2-py3-none-any.whl (3.0 MB)\n","\u001b[K     |████████████████████████████████| 3.0 MB 31.9 MB/s \n","\u001b[?25hCollecting tensorflow-estimator<2.3.0,>=2.2.0\n","  Downloading tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454 kB)\n","\u001b[K     |████████████████████████████████| 454 kB 36.9 MB/s \n","\u001b[?25hCollecting h5py<2.11.0,>=2.10.0\n","  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n","\u001b[K     |████████████████████████████████| 2.9 MB 31.8 MB/s \n","\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (0.37.1)\n","Collecting gast==0.3.3\n","  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.35.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.0.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.8.1)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (57.4.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.3.6)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2.23.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.4.6)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.8)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.2.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.2.4)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.11.3)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.10.0.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.7.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.4.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.24.3)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.2.0)\n","Installing collected packages: tensorflow-estimator, tensorboard, h5py, gast, tensorflow\n","  Attempting uninstall: tensorflow-estimator\n","    Found existing installation: tensorflow-estimator 2.8.0\n","    Uninstalling tensorflow-estimator-2.8.0:\n","      Successfully uninstalled tensorflow-estimator-2.8.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.8.0\n","    Uninstalling tensorboard-2.8.0:\n","      Successfully uninstalled tensorboard-2.8.0\n","  Attempting uninstall: h5py\n","    Found existing installation: h5py 3.1.0\n","    Uninstalling h5py-3.1.0:\n","      Successfully uninstalled h5py-3.1.0\n","  Attempting uninstall: gast\n","    Found existing installation: gast 0.5.3\n","    Uninstalling gast-0.5.3:\n","      Successfully uninstalled gast-0.5.3\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.8.0\n","    Uninstalling tensorflow-2.8.0:\n","      Successfully uninstalled tensorflow-2.8.0\n","Successfully installed gast-0.3.3 h5py-2.10.0 tensorboard-2.2.2 tensorflow-2.2.0 tensorflow-estimator-2.2.0\n"]}],"source":["!pip install keras==2.3.1\n","!pip install tensorflow==2.2.0"]},{"cell_type":"code","source":["import tensorflow as tf\n","tf.__version__"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"qGt0s-t4NGZ1","executionInfo":{"status":"ok","timestamp":1649636938320,"user_tz":-540,"elapsed":2666,"user":{"displayName":"이재우","userId":"15413872261520463322"}},"outputId":"a859a0fb-ccfe-426c-8fcf-63ab77975576"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'2.8.0'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":1}]},{"cell_type":"code","source":["import keras\n","keras.__version__"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":54},"id":"0pRWT1EoNSc0","executionInfo":{"status":"ok","timestamp":1649606481725,"user_tz":-540,"elapsed":419,"user":{"displayName":"이재우","userId":"15413872261520463322"}},"outputId":"643aaf85-7e02-4a32-d385-9581c227c1cd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Using TensorFlow backend.\n"]},{"output_type":"execute_result","data":{"text/plain":["'2.3.1'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","source":["소규모 데이터 셋에서 컨브넷 훈련하기 4000개의 강아지 고양이 사진으로 구성된 데이터셋을 train 2000, validation 1000, test 1000개로 분리하여 사용.\n","- 과대 적합 여부 확인\n","- 데이터 증식을 통한 네트워크의 성능 개선\n","- 사전 훈련된 네트워크을 활용"],"metadata":{"id":"0CnOLgjtNV09"}},{"cell_type":"markdown","source":["## 작은 데이터셋 문제에서 딥러닝의 타당성\n","딥러닝은 데이터가 풍부할 때만 작동한다는 말을 이따금 듣습니다. 부분적으로는 맞습니다. 딥러닝의 근본적인 특징은 훈련 데이터에서 특성 공학의 수작업 없이 흥미로운 특성을 찾을 수 있는 것입니다. 이는 훈련 샘플이 많아야만 가능합니다. 입력 샘플이 이미지와 같이 매우 고차원인 문제에서는 특히 그렇습니다.\n","- 많은 샘플이 의미하는 것은 상대적\n","- 복잡한 문제를 푸는 컨브넷을 수십 개의 샘플만을 사용해서 훈련하는 것은 불가능\n","- 컨브넷은 지역적이고 평행 이동으로 변하지 않는 특성을 학습하기 때문에 지각에 관한 문제에서 매우 효율적으로 데이터를 사용\n","- 매우 작은 이미지 데이터셋에서 어떤 종류의 특성 공학을 사용하지 않고 컨브넷을 처음부터 훈련해도 납득할 만한 결과를 만들 수 있다.\n","- 딥러닝 모델은 태생적으로 매우 다목적\n","- 대규모 데이터셋에서 훈련시킨 이미지 분류 모델이나 스피치-투-텍스트 모델을 조금만 변경해서 완전히 다른 문제에 재사용\n","- 특히 컴퓨터 비전에서는 (보통 ImageNet 데이터셋에서 훈련된) 사전 훈련된 모델들이 다운로드받을 수 있도록 많이 공개되어 있어서 매우 적은 데이터에서 강력한 비전 모델을 만드는데 사용할 수 있다."],"metadata":{"id":"tuz4dNX3NfFV"}},{"cell_type":"markdown","source":["[과제] 2000개의 훈련 이미지, 1000개의 검증 이미지, 1000개의 테스트 이미지를 준비해 주세요."],"metadata":{"id":"p6OvKQrpNtiN"}},{"cell_type":"code","source":["!pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AEEzjtdYNr2R","executionInfo":{"status":"ok","timestamp":1649606595099,"user_tz":-540,"elapsed":440,"user":{"displayName":"이재우","userId":"15413872261520463322"}},"outputId":"f03f23e3-5558-4509-a7ee-09494d08b319"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n"]}]},{"cell_type":"code","source":["import os, shutil"],"metadata":{"id":"Zd14hIe1NcPD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os, shutil\n","# 원본 데이터셋을 압축 해제한 디렉터리 경로\n","original_dataset_dir = '/content/drive/MyDrive/cakd5/m9_딥러닝알고리즘구현/datasets/cats_and_dogs'\n","\n","# 소규모 데이터셋을 저장할 디렉터리\n","base_dir = '/content/drive/MyDrive/cakd5/m9_딥러닝알고리즘구현/datasets/cats_and_dogs_small'\n","if os.path.exists(base_dir):  # 반복적인 실행을 위해 디렉토리를 삭제합니다.\n","    shutil.rmtree(base_dir)   # 이 코드는 책에 포함되어 있지 않습니다. 지정된 폴더, 하위 디렉토리 폴더, 파일 모두 삭제\n","os.mkdir(base_dir)\n","\n","# 훈련, 검증, 테스트 분할을 위한 디렉터리\n","train_dir = os.path.join(base_dir, 'train')\n","os.mkdir(train_dir)\n","validation_dir = os.path.join(base_dir, 'validation')\n","os.mkdir(validation_dir)\n","test_dir = os.path.join(base_dir, 'test')\n","os.mkdir(test_dir)\n","\n","# 훈련용 고양이 사진 디렉터리\n","train_cats_dir = os.path.join(train_dir, 'cats')\n","os.mkdir(train_cats_dir)\n","\n","# 훈련용 강아지 사진 디렉터리\n","train_dogs_dir = os.path.join(train_dir, 'dogs')\n","os.mkdir(train_dogs_dir)\n","\n","# 검증용 고양이 사진 디렉터리\n","validation_cats_dir = os.path.join(validation_dir, 'cats')\n","os.mkdir(validation_cats_dir)\n","\n","# 검증용 강아지 사진 디렉터리\n","validation_dogs_dir = os.path.join(validation_dir, 'dogs')\n","os.mkdir(validation_dogs_dir)\n","\n","# 테스트용 고양이 사진 디렉터리\n","test_cats_dir = os.path.join(test_dir, 'cats')\n","os.mkdir(test_cats_dir)\n","\n","# 테스트용 강아지 사진 디렉터리\n","test_dogs_dir = os.path.join(test_dir, 'dogs')\n","os.mkdir(test_dogs_dir)\n","\n","# 처음 1,000개의 고양이 이미지를 train_cats_dir에 복사합니다\n","fnames = ['cat.{}.jpg'.format(i) for i in range(1000)]\n","for fname in fnames:\n","    src = os.path.join(original_dataset_dir, fname)\n","    dst = os.path.join(train_cats_dir, fname)\n","    shutil.copyfile(src, dst)\n","\n","# 다음 500개 고양이 이미지를 validation_cats_dir에 복사합니다\n","fnames = ['cat.{}.jpg'.format(i) for i in range(1000, 1500)]\n","for fname in fnames:\n","    src = os.path.join(original_dataset_dir, fname)\n","    dst = os.path.join(validation_cats_dir, fname)\n","    shutil.copyfile(src, dst)\n","    \n","# 다음 500개 고양이 이미지를 test_cats_dir에 복사합니다\n","fnames = ['cat.{}.jpg'.format(i) for i in range(1500, 2000)]\n","for fname in fnames:\n","    src = os.path.join(original_dataset_dir, fname)\n","    dst = os.path.join(test_cats_dir, fname)\n","    shutil.copyfile(src, dst)\n","    \n","# 처음 1,000개의 강아지 이미지를 train_dogs_dir에 복사합니다\n","fnames = ['dog.{}.jpg'.format(i) for i in range(1000)]\n","for fname in fnames:\n","    src = os.path.join(original_dataset_dir, fname)\n","    dst = os.path.join(train_dogs_dir, fname)\n","    shutil.copyfile(src, dst)\n","    \n","# 다음 500개 강아지 이미지를 validation_dogs_dir에 복사합니다\n","fnames = ['dog.{}.jpg'.format(i) for i in range(1000, 1500)]\n","for fname in fnames:\n","    src = os.path.join(original_dataset_dir, fname)\n","    dst = os.path.join(validation_dogs_dir, fname)\n","    shutil.copyfile(src, dst)\n","    \n","# 다음 500개 강아지 이미지를 test_dogs_dir에 복사합니다\n","fnames = ['dog.{}.jpg'.format(i) for i in range(1500, 2000)]\n","for fname in fnames:\n","    src = os.path.join(original_dataset_dir, fname)\n","    dst = os.path.join(test_dogs_dir, fname)\n","    shutil.copyfile(src, dst)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":400},"id":"MrqALHsSNzGj","executionInfo":{"status":"error","timestamp":1649630624018,"user_tz":-540,"elapsed":50318,"user":{"displayName":"이재우","userId":"15413872261520463322"}},"outputId":"21a70e5b-54a6-4ac5-dbc6-745ce385b52a"},"execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-ea6fcef440fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mbase_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/cakd5/m9_딥러닝알고리즘구현/datasets/cats_and_dogs_small'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# 반복적인 실행을 위해 디렉토리를 삭제합니다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# 이 코드는 책에 포함되어 있지 않습니다. 지정된 폴더, 하위 디렉토리 폴더, 파일 모두 삭제\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/shutil.py\u001b[0m in \u001b[0;36mrmtree\u001b[0;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[1;32m    496\u001b[0m                     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m                     \u001b[0monerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/shutil.py\u001b[0m in \u001b[0;36mrmtree\u001b[0;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[1;32m    494\u001b[0m                 \u001b[0m_rmtree_safe_fd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monerror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m                     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                     \u001b[0monerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/cakd5/m9_딥러닝알고리즘구현/datasets/cats_and_dogs_small'"]}]},{"cell_type":"code","source":["import os, shutil\n","base_dir = './drive/MyDrive/cakd5/m9_딥러닝알고리즘구현/cats_and_dogs_small'\n","\n","\n","# 훈련, 검증, 테스트 분할을 위한 디렉터리\n","train_dir = os.path.join(base_dir, 'train')\n","\n","validation_dir = os.path.join(base_dir, 'validation')\n","\n","test_dir = os.path.join(base_dir, 'test')\n","\n","\n","# 훈련용 고양이 사진 디렉터리\n","train_cats_dir = os.path.join(train_dir, 'cats')\n","\n","\n","# 훈련용 강아지 사진 디렉터리\n","train_dogs_dir = os.path.join(train_dir, 'dogs')\n","\n","\n","# 검증용 고양이 사진 디렉터리\n","validation_cats_dir = os.path.join(validation_dir, 'cats')\n","\n","\n","# 검증용 강아지 사진 디렉터리\n","validation_dogs_dir = os.path.join(validation_dir, 'dogs')\n","\n","\n","# 테스트용 고양이 사진 디렉터리\n","test_cats_dir = os.path.join(test_dir, 'cats')\n","\n","\n","# 테스트용 강아지 사진 디렉터리\n","test_dogs_dir = os.path.join(test_dir, 'dogs')"],"metadata":{"id":"BuQxf2-nUCus"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('훈련용 고양이 이미지 전체 개수:', len(os.listdir(train_cats_dir)))\n","print('훈련용 강아지 이미지 전체 개수:', len(os.listdir(train_dogs_dir)))\n","print('검증용 고양이 이미지 전체 개수:', len(os.listdir(validation_cats_dir)))\n","print('검증용 강아지 이미지 전체 개수:', len(os.listdir(validation_dogs_dir)))\n","print('테스트용 고양이 이미지 전체 개수:', len(os.listdir(test_cats_dir)))\n","print('테스트용 강아지 이미지 전체 개수:', len(os.listdir(test_dogs_dir)))"],"metadata":{"id":"nhh02V9GUHEU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from keras import layers, models\n","model = models.Sequential()\n","model.add(layers.Conv2D(32, (3,3), activation='relu', input_shape=(150,150,3)))\n","model.add(layers.MaxPooling2D((2,2)))\n","model.add(layers.Conv2D(64,(3,3), activation='relu'))\n","model.add(layers.MaxPooling2D((2,2)))\n","model.add(layers.Conv2D(128,(3,3), activation='relu'))\n","model.add(layers.MaxPooling2D((2,2)))\n","model.add(layers.Conv2D(128,(3,3), activation='relu'))\n","model.add(layers.MaxPooling2D((2,2)))\n","model.add(layers.Flatten())\n","model.add(layers.Dense(512, activation='relu'))\n","model.add(layers.Dense(1,activation='sigmoid'))"],"metadata":{"id":"qsbxXOzCUJUc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.summary()"],"metadata":{"id":"G6nHmgDCULjE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from keras import optimizers\n","\n","model.compile(loss = 'binary_crossentropy',\n","              optimizer=optimizers.RMSprop(lr=1e-4),\n","              metrics=['acc'])"],"metadata":{"id":"ig5h_Bd-UNj9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from keras.preprocessing.image import ImageDataGenerator\n","\n","# 모든 이미지를 1/255로 스케일을 조정합니다\n","train_datagen = ImageDataGenerator(rescale=1./255)\n","test_datagen = ImageDataGenerator(rescale=1./255)\n","\n","train_generator = train_datagen.flow_from_directory(\n","        # 타깃 디렉터리\n","        train_dir,\n","        # 모든 이미지를 150 × 150 크기로 바꿉니다\n","        target_size=(150, 150),\n","        batch_size=20,\n","        # binary_crossentropy 손실을 사용하기 때문에 이진 레이블이 필요합니다\n","        class_mode='binary')\n","\n","validation_generator = test_datagen.flow_from_directory(\n","        validation_dir,\n","        target_size=(150, 150),\n","        batch_size=20,\n","        class_mode='binary')"],"metadata":{"id":"kssILMyTUPV9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for data_batch, labels_batch in train_generator:\n","    print('배치 데이터 크기:', data_batch.shape)\n","    print('배치 레이블 크기:', labels_batch.shape)\n","    break"],"metadata":{"id":"SZ_YOsrTUSTd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["제너레이터를 사용한 데이터에 모델을 훈련\n","- fit_generator 메서드는 fit 메서드와 동일하되 데이터 제너레이터를 사용할 수 있다.\n","- 이 메서드는 첫 번째 매개변수로 입력과 타깃의 배치를 끝없이 반환하는 파이썬 제너레이터를 기대한다.\n","- 데이터가 끝없이 생성되기 때문에 케라스 모델에 하나의 에포크를 정의하기 위해 제너레이터로부터 얼마나 많은 샘플을 뽑을 것인지 알려 주어야 한다.\n","- steps_per_epoch 매개변수에서 이를 설정한다.\n","- 제너레이터로부터 steps_per_epoch 개의 배치만큼 뽑은 다음, 즉 steps_per_epoch 횟수만큼 경사 하강법 단계를 실행한 다음에 훈련 프로세스는 다음 에포크로 넘어간다.\n","- 20개의 샘플이 하나의 배치이므로 2,000개의 샘플을 모두 처리할 때까지 100개의 배치를 뽑을 것이다.\n","- fit_generator를 사용할 때 fit 메서드와 마찬가지로 validation_data 매개변수를 전달할 수 있다.\n","- 이 매개변수에는 데이터 제너레이터도 가능하지만 넘파이 배열의 튜플도 가능하다.\n","- validation_data로 제너레이터를 전달하면 검증 데이터의 배치를 끝없이 반환한다.\n","- 따라서 검증 데이터 제너레이터에서 얼마나 많은 배치를 추출하여 평가할지 validation_steps 매개변수에 지정해야 한다."],"metadata":{"id":"3MRQn9RyUZC6"}},{"cell_type":"code","source":["history = model.fit_generator(\n","    train_generator,\n","    steps_per_epoch=100,\n","    epochs=30,\n","    validation_data=validation_generator,\n","    validation_steps=50\n",")"],"metadata":{"id":"y9unO8lvUUL1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.save('./drive/MyDrive/cakd5/m9_딥러닝알고리즘구현/cats_and_dogs_small_1.h5')"],"metadata":{"id":"Wul_v-51UrWd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from keras.models import load_model\n","model = load_model('./drive/MyDrive/cakd5/m9_딥러닝알고리즘구현/cats_and_dogs_small_1.h5')"],"metadata":{"id":"Wbw1OXAXUtxl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","acc = history.history['acc']\n","val_acc = history.history['val_acc'] \n","loss = history.history['loss'] \n","val_loss = history.history['val_loss'] \n","\n","epochs  = range(len(acc))\n","\n","plt.plot(epochs, acc, 'bo', label='Training acc')\n","plt.plot(epochs, val_acc, 'b', label = 'Validation acc')\n","plt.title('Training and validation accuracy')\n","plt.legend()\n","\n","plt.figure()\n","\n","plt.plot(epochs, loss, 'bo', label='Training loss')\n","plt.plot(epochs, val_loss, 'b', label = 'Validation loss')\n","plt.title('Training and validation loss')\n","plt.legend()\n","\n","plt.show()\n"],"metadata":{"id":"1wWtvvdCUwyF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["https://ichi.pro/ko/keras-mich-tensorflowleul-sayonghan-imiji-deiteo-jeungdae-tamsaeg-184813206747204\n"],"metadata":{"id":"QtMrjtbmHCTj"}},{"cell_type":"markdown","source":["[과제] 데이터 증식하여 모델 학습, 평가 및 시각화"],"metadata":{"id":"5I_ryn7ZKMh6"}},{"cell_type":"code","source":[""],"metadata":{"id":"mTONoZqYUz5E"},"execution_count":null,"outputs":[]}]}