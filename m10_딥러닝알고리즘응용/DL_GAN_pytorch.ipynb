{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DL_GAN_pytorch.ipynb","provenance":[],"mount_file_id":"1O8p-z6reW0EFphEwDE5pfoJH433Xz6sG","authorship_tag":"ABX9TyOHyBaAyPB2z7h7Q5yGo+tb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":17,"metadata":{"id":"92H657GJxGEy","executionInfo":{"status":"ok","timestamp":1652929228695,"user_tz":-540,"elapsed":1004,"user":{"displayName":"이재우","userId":"15413872261520463322"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torchvision import datasets\n","import torchvision.transforms as transforms\n","from torchvision.utils import save_image"]},{"cell_type":"code","source":["latent_dim = 100\n","\n","# 생성자 클래스 정의\n","class Generator(nn.Module):\n","  def __init__(self):\n","    super(Generator, self).__init__()\n","\n","    # 하나의 블록(block) 정의\n","    def block(input_dim, output_dim, normalize = True):\n","      layers = [nn.Linear(input_dim, output_dim)]\n","      if normalize:\n","        layers.append(nn.BatchNorm1d(output_dim,0.8)) # 배치 정규화 수행(차원 동일)\n","      layers.append(nn.LeakyReLU(0.2,inplace=True))\n","      return layers\n","\n","    # 생성자 모델은 연속적인 여러개의 블록을 가짐\n","    self.model = nn.Sequential(\n","        *block(latent_dim,128,normalize=False),\n","        *block(128,256),\n","        *block(256,512),\n","        *block(512,1024),\n","        nn.Linear(1024,1*28*28),\n","        nn.Tanh()\n","    )\n","\n","  def forward(self,z):\n","    img = self.model(z) # 노이즈 벡터 z\n","    # 이미지 형태로 만들어줌\n","    img = img.view(img.size(0),1,28,28) # 배치 사이즈, 채널, 넓이 , 높이\n","    return img"],"metadata":{"id":"mnCyNamDxsuj","executionInfo":{"status":"ok","timestamp":1652929231813,"user_tz":-540,"elapsed":2,"user":{"displayName":"이재우","userId":"15413872261520463322"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["# 판별자 클래스 정의\n","class Discriminator(nn.Module):\n","  def __init__(self):\n","    super(Discriminator,self).__init__()\n","\n","    self.model = nn.Sequential(\n","        nn.Linear(1,28*28,512),\n","        nn.LeakyReLU(0.2, inplace=True),\n","        nn.Linear(512,256),\n","        nn.LeakyReLU(0.2,inplace=True),\n","        nn.Linear(256,1),\n","        nn.Sigmoid() # 확률값을 내보냄\n","    )\n","\n","    # 이미지에 대한 판별 결과를 반환\n","  def forward(self,img):\n","    flattened = img.view(img.size(0),-1)\n","    output=self.model(flattened)\n","    return output"],"metadata":{"id":"qW4BomLz1zrD","executionInfo":{"status":"ok","timestamp":1652929238245,"user_tz":-540,"elapsed":676,"user":{"displayName":"이재우","userId":"15413872261520463322"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["# 학습 데이터셋 불러오기\n","transforms_train = transforms.Compose(\n","    [transforms.Resize(28),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.5],[0.5])]\n",")\n","train_dataset = datasets.MNIST(root='./dataset',train=True, download=True, transform = transforms_train)\n","dataloader = torch.utils.data.DataLoader(train_dataset, batch_size = 128,shuffle=True,num_workers=4)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h8cL3IWM3Q1-","executionInfo":{"status":"ok","timestamp":1652929241095,"user_tz":-540,"elapsed":487,"user":{"displayName":"이재우","userId":"15413872261520463322"}},"outputId":"9a2d95f4-1cd8-4cd7-a3e0-d7426f2b0f86"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]}]},{"cell_type":"code","source":["from torch.nn.modules.activation import LeakyReLU\n","#생성자 판별자 초기화\n","generator = Generator()\n","discriminator = Discriminator()\n","\n","generator.cuda()\n","discriminator.cuda()\n","\n","#손실 함수\n","adversarial_loss = nn.BCELoss()\n","adversarial_loss.cuda()\n","#학습률 설정\n","lr = 0.0002\n","\n","# 최적화 함수\n","optimizer_G = torch.optim.Adam(generator.parameters(),lr= lr,betas = (0.5,0.999))\n","\n","optimizer_D = torch.optim.Adam(discriminator.parameters(),lr = lr,betas = (0.5,0.999))"],"metadata":{"id":"Skh6ok8R4cf6","executionInfo":{"status":"ok","timestamp":1652929249923,"user_tz":-540,"elapsed":394,"user":{"displayName":"이재우","userId":"15413872261520463322"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["import time\n","n_epochs = 200\n","sample_interval = 2000\n","start_time = time.time()\n","\n","for epoch in range(n_epochs):\n","  for i, (imgs,_) in enumerate(dataloader):\n","    # 진짜 이미지와 가짜 이미지에 대한 정답 레이블 생성\n","    real = torch.cuda.FloatTensor(imgs.size(0),1).fill_(1.0)\n","    fake = torch.cuda.FloatTensor(imgs.size(0),1).fill_(0.0)\n","\n","    real_imgs = imgs.cuda()\n","\n","    # 생성자 학습\n","    optimizer_G.zero_grad()\n","\n","    # 랜덤 노이즈 샘플링\n","    z = torch.normal(mean=0,std=1,size = (imgs.shape[0],latent_dim)).cuda()\n","\n","    # 이미지 생성\n","    generated_imgs = generator(z)\n","\n","    # 생성자의 손실 값 계산\n","    g_loss = adversarial_loss(discriminator(generated_imgs),real)\n","\n","    # 생성자 업데이트\n","    g_loss.backward()\n","    optimizer_G.step()\n","\n","    # 판별자 학습\n","    optimizer_D.zero_grad()\n","\n","    # 판별자 손실 값 계산\n","    real_loss = adversarial_loss(discriminator(real_imgs),real)\n","    fake_loss = adversarial_loss(discriminator(generated_imgs.detach()),fake)\n","    d_loss = (real_loss + fake_loss)/2\n","\n","    # 판별자 업데이트\n","    d_loss.backward()\n","    optimizer_D.step()\n","\n","    done = epoch*len(dataloader) + i\n","    if done % sample_interval == 0:\n","      #생성된 이미지 중에서 25개만 선택하여 5x5 격자 이미지에 출력\n","      save_image(generated_imgs.data[:25], f'{done}.png',nrows=5, normalize=True)\n","\n","  # 하나의 epoch가 끝날 때마다 log 출력\n","  print(f'[Epoch {epoch}/{n_epochs}] [D loss: {d_loss.item():.6f}] [G loss: {g_loss.item():.6f}] [Elapsed time: {time.time() - start_time:.2f}s]')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":457},"id":"FLYKPr5b5f_E","executionInfo":{"status":"error","timestamp":1652929264090,"user_tz":-540,"elapsed":964,"user":{"displayName":"이재우","userId":"15413872261520463322"}},"outputId":"04637222-c183-4032-e3d4-5325f822d7a6"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-153764589004>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# 생성자의 손실 값 계산\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madversarial_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_imgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# 생성자 업데이트\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-19-cb4b6fd575da>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mflattened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflattened\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (128x784 and 1x784)"]}]},{"cell_type":"code","source":["from IPython.display import Image\n","Image('92000.png')"],"metadata":{"id":"0UZdxGEp9bxA"},"execution_count":null,"outputs":[]}]}